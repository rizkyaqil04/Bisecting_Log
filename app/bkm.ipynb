{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15503fe2",
   "metadata": {},
   "source": [
    "# Notebook: Sistem Pendukung Investigasi Serangan Siber Berdasarkan Log Server Web dan Bisecting K-Means\n",
    "\n",
    "Notebook ini membangun sistem analisis log server web untuk investigasi serangan siber, dengan pipeline:\n",
    "1. Import library & setup lingkungan\n",
    "2. Membaca & membersihkan data log\n",
    "3. Ekstraksi & tokenisasi URL\n",
    "4. Pembuatan embedding URL dengan BERT\n",
    "5. Clustering URL menggunakan Bisecting K-Means\n",
    "6. Visualisasi & analisis hasil cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab79c3",
   "metadata": {},
   "source": [
    "## 1. Import Library dan Setup Lingkungan\n",
    "\n",
    "Impor semua library yang diperlukan (pandas, numpy, torch, transformers, sklearn, dsb). Lakukan setup device (CPU/GPU) dan inisialisasi model/tokenizer BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca98507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, unquote\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from gensim.models import FastText\n",
    "from sklearn.cluster import KMeans\n",
    "from custom_bkm import VerboseBisectingKMeans, flush_print\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from decoder import parse_dec_file_to_dataframe\n",
    "from pprint import pprint\n",
    "from tqdm import trange\n",
    "from sklearn.feature_extraction.text import (\n",
    "    HashingVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    TfidfTransformer,\n",
    ")\n",
    "from scipy.sparse import csr_matrix, hstack, vstack, issparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ecad055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444b3bd62f574e929f59f13663a8a9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a147d8894bc45bcb8d1323c5b0a9ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a727f027991145f295fe851fc61ab78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0687345def4b4066808e4a895cdfd5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer and model ONCE globally for efficiency\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "MODEL = BertModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "MODEL.eval()\n",
    "\n",
    "# Important Variables\n",
    "input_log_file = \"../inputs/sample_1000000.log\"\n",
    "output_dir = \"../outputs/sample_1000000.csv\"\n",
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737a45c",
   "metadata": {},
   "source": [
    "## 2. Membaca dan Membersihkan Data Log\n",
    "\n",
    "Baca file log hasil decoding (menggunakan fungsi dari `decoder.py`), filter bot, dan parsing menjadi DataFrame yang siap diolah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf659046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 896359 rows from ../inputs/sample_1000000.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>time</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>protocol</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>referrer</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>extra</th>\n",
       "      <th>no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.217.188.22</td>\n",
       "      <td>2019-01-26 15:49:54+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/static/images/guarantees/warranty.png</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>5807</td>\n",
       "      <td>https://www.zanbil.ir/m/index?utm_medium=26&amp;ut...</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 8.1.0; SAMSUNG SM-...</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.199.139.223</td>\n",
       "      <td>2019-01-25 18:50:35+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/image/30970?name=vb-8320h-1.jpg&amp;wh=200x200</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>8788</td>\n",
       "      <td>-</td>\n",
       "      <td>Dalvik/2.1.0 (Linux; U; Android 7.0; SM-A510F ...</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.209.107.210</td>\n",
       "      <td>2019-01-26 12:28:46+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/image/61997/productModel/150x150</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>2746</td>\n",
       "      <td>https://www.zanbil.ir/filter/p3?page=1</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...</td>\n",
       "      <td>-</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>217.219.52.1</td>\n",
       "      <td>2019-01-24 05:43:19+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/site/searchAutoComplete?f=p49&amp;f=b63,stexists&amp;...</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zanbil.ir/filter?f=p49,b63,stexists</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.185.151.126</td>\n",
       "      <td>2019-01-23 10:40:42+00:00</td>\n",
       "      <td>GET</td>\n",
       "      <td>/image/8165/productModel/150x150</td>\n",
       "      <td>HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>3247</td>\n",
       "      <td>https://www.zanbil.ir/browse/kitchen-sink/سینک...</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...</td>\n",
       "      <td>-</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ip                      time method  \\\n",
       "0    5.217.188.22 2019-01-26 15:49:54+00:00    GET   \n",
       "1  89.199.139.223 2019-01-25 18:50:35+00:00    GET   \n",
       "2  46.209.107.210 2019-01-26 12:28:46+00:00    GET   \n",
       "3    217.219.52.1 2019-01-24 05:43:19+00:00    GET   \n",
       "4  91.185.151.126 2019-01-23 10:40:42+00:00    GET   \n",
       "\n",
       "                                                 url  protocol  status  size  \\\n",
       "0             /static/images/guarantees/warranty.png  HTTP/1.1     200  5807   \n",
       "1        /image/30970?name=vb-8320h-1.jpg&wh=200x200  HTTP/1.1     200  8788   \n",
       "2                  /image/61997/productModel/150x150  HTTP/1.1     200  2746   \n",
       "3  /site/searchAutoComplete?f=p49&f=b63,stexists&...  HTTP/1.1     302     0   \n",
       "4                   /image/8165/productModel/150x150  HTTP/1.1     200  3247   \n",
       "\n",
       "                                            referrer  \\\n",
       "0  https://www.zanbil.ir/m/index?utm_medium=26&ut...   \n",
       "1                                                  -   \n",
       "2             https://www.zanbil.ir/filter/p3?page=1   \n",
       "3    https://www.zanbil.ir/filter?f=p49,b63,stexists   \n",
       "4  https://www.zanbil.ir/browse/kitchen-sink/سینک...   \n",
       "\n",
       "                                          user_agent extra  no  \n",
       "0  Mozilla/5.0 (Linux; Android 8.1.0; SAMSUNG SM-...     -   1  \n",
       "1  Dalvik/2.1.0 (Linux; U; Android 7.0; SM-A510F ...     -   2  \n",
       "2  Mozilla/5.0 (Windows NT 6.1; rv:64.0) Gecko/20...     -   3  \n",
       "3  Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...     -   4  \n",
       "4  Mozilla/5.0 (Windows NT 6.1; Win64; x64) Apple...     -   5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process log file\n",
    "df = parse_dec_file_to_dataframe(input_log_file)\n",
    "print(f\"✅ Loaded {len(df)} rows from {input_log_file}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ebe8843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DataFrame Info ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 896359 entries, 0 to 896358\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count   Dtype              \n",
      "---  ------      --------------   -----              \n",
      " 0   ip          896359 non-null  object             \n",
      " 1   time        896359 non-null  datetime64[ns, UTC]\n",
      " 2   method      896359 non-null  object             \n",
      " 3   url         896359 non-null  object             \n",
      " 4   protocol    896359 non-null  object             \n",
      " 5   status      896359 non-null  int64              \n",
      " 6   size        896359 non-null  int64              \n",
      " 7   referrer    896359 non-null  object             \n",
      " 8   user_agent  896359 non-null  object             \n",
      " 9   extra       896359 non-null  object             \n",
      " 10  no          896359 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(3), object(7)\n",
      "memory usage: 75.2+ MB\n",
      "\n",
      "=== DataFrame Describe ===\n",
      "                     ip                                 time  method  \\\n",
      "count            896359                               896359  896359   \n",
      "unique           123534                                  NaN       5   \n",
      "top     151.239.241.163                                  NaN     GET   \n",
      "freq               9125                                  NaN  879601   \n",
      "mean                NaN  2019-01-24 09:49:18.017156864+00:00     NaN   \n",
      "min                 NaN            2019-01-22 00:26:16+00:00     NaN   \n",
      "25%                 NaN            2019-01-23 06:13:44+00:00     NaN   \n",
      "50%                 NaN            2019-01-24 08:46:54+00:00     NaN   \n",
      "75%                 NaN     2019-01-25 17:32:48.500000+00:00     NaN   \n",
      "max                 NaN            2019-01-26 16:59:13+00:00     NaN   \n",
      "std                 NaN                                  NaN     NaN   \n",
      "\n",
      "                   url  protocol         status          size referrer  \\\n",
      "count           896359    896359  896359.000000  8.963590e+05   896359   \n",
      "unique          103210         6            NaN           NaN    51656   \n",
      "top     /settings/logo  HTTP/1.1            NaN           NaN        -   \n",
      "freq             34135    896260            NaN           NaN    54966   \n",
      "mean               NaN       NaN     206.788211  1.165259e+04      NaN   \n",
      "min                NaN       NaN     200.000000  0.000000e+00      NaN   \n",
      "25%                NaN       NaN     200.000000  2.116000e+03      NaN   \n",
      "50%                NaN       NaN     200.000000  4.011000e+03      NaN   \n",
      "75%                NaN       NaN     200.000000  7.963000e+03      NaN   \n",
      "max                NaN       NaN     504.000000  1.126965e+06      NaN   \n",
      "std                NaN       NaN      35.650587  2.888088e+04      NaN   \n",
      "\n",
      "                                               user_agent   extra  \\\n",
      "count                                              896359  896359   \n",
      "unique                                              17673     688   \n",
      "top     Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...       -   \n",
      "freq                                                71961  894135   \n",
      "mean                                                  NaN     NaN   \n",
      "min                                                   NaN     NaN   \n",
      "25%                                                   NaN     NaN   \n",
      "50%                                                   NaN     NaN   \n",
      "75%                                                   NaN     NaN   \n",
      "max                                                   NaN     NaN   \n",
      "std                                                   NaN     NaN   \n",
      "\n",
      "                    no  \n",
      "count    896359.000000  \n",
      "unique             NaN  \n",
      "top                NaN  \n",
      "freq               NaN  \n",
      "mean     500014.988029  \n",
      "min           1.000000  \n",
      "25%      249883.500000  \n",
      "50%      500204.000000  \n",
      "75%      749869.500000  \n",
      "max     1000000.000000  \n",
      "std      288686.300372  \n",
      "\n",
      "=== DataFrame Columns ===\n",
      "Index(['ip', 'time', 'method', 'url', 'protocol', 'status', 'size', 'referrer',\n",
      "       'user_agent', 'extra', 'no'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DataFrame Info ===\")\n",
    "df.info()\n",
    "print(\"\\n=== DataFrame Describe ===\")\n",
    "print(df.describe(include='all'))\n",
    "print(\"\\n=== DataFrame Columns ===\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c1cc1",
   "metadata": {},
   "source": [
    "## 3. Ekstraksi dan Tokenisasi Fitur\n",
    "\n",
    "Ekstrak fitur dari log, lakukan masking angka pada fitur URL, kategorisasi fitur status, dan tokenisasi path serta query string menjadi token-token teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc8d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_numbers(url):\n",
    "    \"\"\"\n",
    "    Replace numeric sequences in a URL with a placeholder token <NUM>.\n",
    "\n",
    "    Args:\n",
    "        url (str): A URL string that may contain numeric sequences.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL string where all numeric sequences are replaced with <NUM>.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\d+', '<NUM>', url)\n",
    "\n",
    "\n",
    "\n",
    "def split_url_tokens(url):\n",
    "    \"\"\"\n",
    "    Tokenize a URL by splitting its path and query string.\n",
    "\n",
    "    Args:\n",
    "        url (str): A URL string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list: List of token strings extracted from the URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    path = unquote(parsed.path)\n",
    "    query = unquote(parsed.query)\n",
    "    delimiters = r\"[\\/\\-\\_\\=\\&\\?\\.\\+\\(\\)\\[\\]\\<\\>\\{\\}]\"\n",
    "    tokens = re.split(delimiters, path.strip(\"/\")) + re.split(delimiters, query)\n",
    "    return [tok for tok in tokens if tok]\n",
    "\n",
    "\n",
    "def tokenize_user_agent(ua):\n",
    "    \"\"\"\n",
    "    Tokenize a User-Agent string by splitting on common delimiters.\n",
    "\n",
    "    Args:\n",
    "        ua (str): User-Agent string.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tokens such as browser, OS, and engine identifiers.\n",
    "    \"\"\"\n",
    "    tokens = re.split(r\"[ /;()]+\", ua)\n",
    "    return [tok for tok in tokens if tok]\n",
    "\n",
    "\n",
    "def categorize_status(code):\n",
    "    \"\"\"\n",
    "    Categorize HTTP status codes into standard ranges.\n",
    "\n",
    "    Args:\n",
    "        code (int): HTTP status code.\n",
    "\n",
    "    Returns:\n",
    "        str: Category label (\"2xx\", \"3xx\", \"4xx\", \"5xx\", or \"other\").\n",
    "    \"\"\"\n",
    "    if 200 <= code < 300:\n",
    "        return \"2xx\"\n",
    "    elif 300 <= code < 400:\n",
    "        return \"3xx\"\n",
    "    elif 400 <= code < 500:\n",
    "        return \"4xx\"\n",
    "    elif 500 <= code < 600:\n",
    "        return \"5xx\"\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a7139c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ tokenized_urls (head & tail)\n",
      "['static images guarantees warranty png',\n",
      " 'image NUM name vb NUM h NUM jpg wh NUM x NUM',\n",
      " 'image NUM productModel NUM x NUM',\n",
      " 'site searchAutoComplete f p NUM f b NUM ,stexists phrase چرخ خیاطی ژانومه '\n",
      " 'مدل NUM',\n",
      " 'image NUM productModel NUM x NUM',\n",
      " '...',\n",
      " 'image NUM productModel NUM x NUM',\n",
      " 'product NUM NUM صندلی مدیریتی نیلپر مدل SM NUM',\n",
      " 'image NUM productModel NUM x NUM',\n",
      " 'image NUM productModel NUM x NUM',\n",
      " 'image NUM productTypeMenu']\n",
      "\n",
      "✅ methods (head)\n",
      "['GET', 'GET', 'GET', 'GET', 'GET']\n",
      "\n",
      "✅ status_categories (head)\n",
      "['2xx', '2xx', '2xx', '3xx', '2xx']\n",
      "\n",
      "✅ sizes (head)\n",
      "[5807, 8788, 2746, 0, 3247]\n",
      "\n",
      "✅ user_agent tokens (head)\n",
      "['Mozilla 5.0 Linux Android 8.1.0 SAMSUNG SM-J730F Build M1AJQ AppleWebKit '\n",
      " '537.36 KHTML, like Gecko SamsungBrowser 8.2 Chrome 63.0.3239.111 Mobile '\n",
      " 'Safari 537.36',\n",
      " 'Dalvik 2.1.0 Linux U Android 7.0 SM-A510F Build NRD90M',\n",
      " 'Mozilla 5.0 Windows NT 6.1 rv:64.0 Gecko 20100101 Firefox 64.0']\n"
     ]
    }
   ],
   "source": [
    "# --- Tokenisasi ---\n",
    "\n",
    "# tokenized_urls = [\" \".join(split_url_tokens(url)) for url in unique_urls]\n",
    "tokenized_urls = [\" \".join(split_url_tokens(mask_numbers(url))) for url in df['url']]\n",
    "print(\"\\n✅ tokenized_urls (head & tail)\")\n",
    "pprint(tokenized_urls[:5] + [\"...\"] + tokenized_urls[-5:])\n",
    "\n",
    "# Method\n",
    "methods = df['method'].tolist()\n",
    "print(\"\\n✅ methods (head)\") \n",
    "pprint(methods[:5])\n",
    "\n",
    "# Status kategori\n",
    "status_categories = df['status'].apply(categorize_status).tolist()\n",
    "print(\"\\n✅ status_categories (head)\")\n",
    "pprint(status_categories[:5])\n",
    "\n",
    "# Size mentah (akan dinormalisasi nanti)\n",
    "sizes = df['size'].tolist()\n",
    "print(\"\\n✅ sizes (head)\")\n",
    "pprint(sizes[:5])\n",
    "\n",
    "# User-Agent token\n",
    "ua_tokens = [\" \".join(tokenize_user_agent(ua)) for ua in df['user_agent']]\n",
    "print(\"\\n✅ user_agent tokens (head)\")\n",
    "pprint(ua_tokens[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297dfb39",
   "metadata": {},
   "source": [
    "## 4. Vektorisasi Fitur\n",
    "\n",
    "Konversi token-token URL menjadi embedding vektor menggunakan berbagai metode, encoding metode dan status, normalisasi size, dan vektorisasi tfidf untuk user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff0d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url_hashing(url_list, n_features=1024, batch_size=50000):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for URLs using a hashing trick.\n",
    "\n",
    "    Args:\n",
    "        url_list (list): List of tokenized URL strings.\n",
    "        n_features (int, optional): Number of output features. Defaults to 1024.\n",
    "        batch_size (int, optional): Number of URLs per batch. Defaults to 50000.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: L2-normalized sparse matrix of hashed URL features.\n",
    "    \"\"\"\n",
    "    vectorizer = HashingVectorizer(\n",
    "        n_features=n_features,\n",
    "        alternate_sign=False,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    X_batches = []\n",
    "    for i in range(0, len(url_list), batch_size):\n",
    "        batch = url_list[i:i + batch_size]\n",
    "        X_batches.append(vectorizer.transform(batch))\n",
    "    X = vstack(X_batches)\n",
    "    return normalize(X, norm='l2', copy=False)\n",
    "\n",
    "\n",
    "def generate_url_tfidf(url_list, n_features=1024, batch_size=50000):\n",
    "    \"\"\"\n",
    "    Generate scalable TF-IDF vectors for URLs using a hashing + IDF approach.\n",
    "\n",
    "    Args:\n",
    "        url_list (list): List of tokenized URL strings.\n",
    "        n_features (int, optional): Number of output features. Defaults to 1024.\n",
    "        batch_size (int, optional): Number of URLs per batch. Defaults to 50000.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Sparse TF-IDF feature matrix for URLs.\n",
    "    \"\"\"\n",
    "    hv = HashingVectorizer(n_features=n_features, alternate_sign=False, dtype=np.float32)\n",
    "\n",
    "    tf_batches = []\n",
    "    for i in range(0, len(url_list), batch_size):\n",
    "        batch = url_list[i:i + batch_size]\n",
    "        tf_batches.append(hv.transform(batch))\n",
    "    X_tf = vstack(tf_batches)\n",
    "\n",
    "    transformer = TfidfTransformer()\n",
    "    X_tfidf = transformer.fit_transform(X_tf)\n",
    "    return normalize(X_tfidf, norm='l2', copy=False)\n",
    "\n",
    "\n",
    "def generate_url_bert(url_list, TOKENIZER, MODEL, device, batch_size=32, out_path=None):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for a list of preprocessed URL strings.\n",
    "\n",
    "    Args:\n",
    "        url_list (list): List of preprocessed URL strings.\n",
    "        TOKENIZER (transformers tokenizer): Pre-trained tokenizer.\n",
    "        MODEL (transformers model): Pre-trained model (e.g. BERT, MiniLM, etc.).\n",
    "        device (torch.device): Computation device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int, optional): Batch size for embedding. Defaults to 32.\n",
    "        out_path (str, optional): Path for memmap file to store embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray or np.memmap: Dense array or memory-mapped embeddings.\n",
    "    \"\"\"\n",
    "    MODEL.eval()\n",
    "    dim = MODEL.config.hidden_size\n",
    "\n",
    "    if out_path:\n",
    "        fp = np.memmap(out_path, dtype=np.float32, mode='w+', shape=(len(url_list), dim))\n",
    "    else:\n",
    "        fp = []\n",
    "\n",
    "    for i in trange(0, len(url_list), batch_size, desc=\"Embedding URLs\"):\n",
    "        batch = url_list[i:i + batch_size]\n",
    "        inputs = TOKENIZER(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = MODEL(**inputs)\n",
    "        emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy().astype(np.float32)\n",
    "\n",
    "        if out_path:\n",
    "            fp[i:i + len(batch)] = emb\n",
    "        else:\n",
    "            fp.append(emb)\n",
    "\n",
    "        del emb, inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if out_path:\n",
    "        del fp\n",
    "        return np.memmap(out_path, dtype=np.float32, mode='r', shape=(len(url_list), dim))\n",
    "    else:\n",
    "        return np.vstack(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a467f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_methods(methods):\n",
    "    \"\"\"\n",
    "    One-hot encode HTTP methods (e.g., GET, POST, PUT) as sparse vectors.\n",
    "\n",
    "    Args:\n",
    "        methods (list[str]): List of HTTP method strings.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: One-hot encoded sparse matrix for methods.\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(sparse_output=True, dtype=np.float32)\n",
    "    return enc.fit_transform(np.array(methods).reshape(-1, 1))\n",
    "\n",
    "\n",
    "def encode_statuses(status_categories):\n",
    "    \"\"\"\n",
    "    One-hot encode HTTP status code categories (e.g., 2xx, 3xx, 4xx, 5xx).\n",
    "\n",
    "    Args:\n",
    "        status_categories (list[str]): List of status category labels.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: One-hot encoded sparse matrix for statuses.\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder(sparse_output=True, dtype=np.float32)\n",
    "    return enc.fit_transform(np.array(status_categories).reshape(-1, 1))\n",
    "\n",
    "\n",
    "def normalize_sizes(sizes):\n",
    "    \"\"\"\n",
    "    Normalize response sizes to the [0, 1] range and return as sparse matrix.\n",
    "\n",
    "    Args:\n",
    "        sizes (list[int]): List of response sizes.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Normalized response sizes.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    arr = scaler.fit_transform(np.array(sizes, dtype=np.float32).reshape(-1, 1))\n",
    "    return csr_matrix(arr)\n",
    "\n",
    "\n",
    "def vectorize_user_agents(ua_tokens, max_features=200):\n",
    "    \"\"\"\n",
    "    Convert tokenized User-Agent strings into TF-IDF sparse vectors.\n",
    "\n",
    "    Args:\n",
    "        ua_tokens (list[str]): List of tokenized User-Agent strings.\n",
    "        max_features (int, optional): Maximum vocabulary size. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: TF-IDF feature matrix for User-Agent tokens.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, dtype=np.float32)\n",
    "    return vectorizer.fit_transform(ua_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8f02375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(*arrays):\n",
    "    \"\"\"\n",
    "    Safely combine multiple feature matrices (sparse or dense) into one matrix.\n",
    "\n",
    "    Automatically converts all dense matrices to sparse if any input is sparse,\n",
    "    ensuring compatibility and efficient memory usage for large datasets.\n",
    "\n",
    "    Args:\n",
    "        *arrays: Variable-length list of matrices (np.ndarray or csr_matrix).\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix or np.ndarray: Combined feature matrix.\n",
    "    \"\"\"\n",
    "    arrays = [a for a in arrays if a is not None]\n",
    "    if any(issparse(a) for a in arrays):\n",
    "        arrays = [csr_matrix(a) if not issparse(a) else a for a in arrays]\n",
    "        return hstack(arrays)\n",
    "    return np.hstack(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4a69663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding URLs:   0%|          | 0/28012 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding URLs: 100%|██████████| 28012/28012 [09:10<00:00, 50.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ final_features shape: (896359, 594)\n"
     ]
    }
   ],
   "source": [
    "# URL embeddings (pilih salah satu)\n",
    "# url_embeddings = generate_url_hashing(tokenized_urls)\n",
    "# url_embeddings = generate_url_tfidf(tokenized_urls)\n",
    "url_embeddings = generate_url_bert(tokenized_urls, TOKENIZER, MODEL, device)\n",
    "\n",
    "if not issparse(url_embeddings):\n",
    "    url_embeddings = normalize(url_embeddings, norm='l2', copy=False)\n",
    "\n",
    "# Fitur tambahan\n",
    "method_enc = encode_methods(df[\"method\"])\n",
    "status_enc = encode_statuses(df[\"status\"].apply(categorize_status))\n",
    "size_enc = normalize_sizes(df[\"size\"])\n",
    "ua_tokens = [\" \".join(tokenize_user_agent(ua)) for ua in df[\"user_agent\"]]\n",
    "ua_enc = vectorize_user_agents(ua_tokens)\n",
    "\n",
    "# Gabungkan semua fitur jadi satu matriks\n",
    "final_features = combine_features(url_embeddings, method_enc, status_enc, size_enc, ua_enc)\n",
    "print(f\"✅ final_features shape: {final_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a485d",
   "metadata": {},
   "source": [
    "## 5. Clustering Log Menggunakan Bisecting K-Means (Library dan Manual)\n",
    "\n",
    "Implementasikan dan jalankan algoritma Bisecting K-Means hasil final fitur, lalu kelompokkan berdasarkan hasil cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60d40831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clusters(features, labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using standard metrics (safe for large/sparse data).\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray or scipy.sparse matrix): Feature matrix.\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with metric results.\n",
    "    \"\"\"\n",
    "    n_labels = len(set(labels))\n",
    "    if n_labels < 2:\n",
    "        return {\n",
    "            \"silhouette\": None,\n",
    "            \"davies_bouldin\": None,\n",
    "            \"calinski_harabasz\": None\n",
    "        }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1️⃣ Silhouette score (supports sparse input)\n",
    "    try:\n",
    "        results[\"silhouette\"] = silhouette_score(features, labels, sample_size=20000 if len(labels) > 20000 else None)\n",
    "    except Exception as e:\n",
    "        results[\"silhouette\"] = None\n",
    "        print(f\"⚠️ Silhouette score skipped: {e}\")\n",
    "\n",
    "    # 2️⃣ Davies–Bouldin and Calinski–Harabasz require dense data\n",
    "    if issparse(features):\n",
    "        # Avoid full toarray() for large datasets (could eat 10+ GB)\n",
    "        if features.shape[0] * features.shape[1] < 50_000 * 1500:\n",
    "            print(\"ℹ️ Converting sparse matrix to dense for small sample...\")\n",
    "            X_dense = features.toarray()\n",
    "        else:\n",
    "            print(\"⚠️ Skipping Davies-Bouldin & Calinski-Harabasz (too large or sparse).\")\n",
    "            results[\"davies_bouldin\"] = None\n",
    "            results[\"calinski_harabasz\"] = None\n",
    "            return results\n",
    "    else:\n",
    "        X_dense = features\n",
    "\n",
    "    # 3️⃣ Compute remaining metrics safely\n",
    "    try:\n",
    "        results[\"davies_bouldin\"] = davies_bouldin_score(X_dense, labels)\n",
    "    except Exception as e:\n",
    "        results[\"davies_bouldin\"] = None\n",
    "        print(f\"⚠️ Davies-Bouldin skipped: {e}\")\n",
    "\n",
    "    try:\n",
    "        results[\"calinski_harabasz\"] = calinski_harabasz_score(X_dense, labels)\n",
    "    except Exception as e:\n",
    "        results[\"calinski_harabasz\"] = None\n",
    "        print(f\"⚠️ Calinski-Harabasz skipped: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def visualize_clusters(features, labels, out_file=\"clusters.png\", save_plot=True, title=None):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA 2D projection.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Feature matrix.\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "        out_file (str): Path to save PNG plot.\n",
    "        save_plot (bool): If True, save plot to file.\n",
    "        title (str): Optional title for the plot.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    reduced = pca.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(reduced[:,0], reduced[:,1], c=labels, cmap=\"tab10\", alpha=0.6)\n",
    "    plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    else:\n",
    "        plt.title(\"Cluster Visualization (PCA)\")\n",
    "    if save_plot:\n",
    "        plt.savefig(out_file)\n",
    "        print(f\"✅ Cluster visualization saved to {out_file}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63944b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting clustering process...\n",
      "STATUS: Starting BisectingKMeans initialization\n",
      "PROGRESS: 7\n",
      "PROGRESS: 14\n",
      "STATUS: KMeans algorithm backend configured\n",
      "PROGRESS: 21\n",
      "STATUS: Data centering complete\n",
      "PROGRESS: 29\n",
      "STATUS: Root cluster created\n",
      "STATUS: Bisecting cluster 1 of 7 (size=896359)\n",
      "PROGRESS: 36\n",
      "STATUS: Bisecting cluster 2 of 7 (size=731426)\n",
      "PROGRESS: 43\n",
      "STATUS: Bisecting cluster 3 of 7 (size=491421)\n",
      "PROGRESS: 50\n",
      "STATUS: Bisecting cluster 4 of 7 (size=319857)\n",
      "PROGRESS: 57\n",
      "STATUS: Bisecting cluster 5 of 7 (size=296085)\n",
      "PROGRESS: 64\n",
      "STATUS: Bisecting cluster 6 of 7 (size=180161)\n",
      "PROGRESS: 71\n",
      "STATUS: Bisecting cluster 7 of 7 (size=115924)\n",
      "PROGRESS: 79\n",
      "PROGRESS: 86\n",
      "STATUS: Cluster labels and centers aggregated\n",
      "PROGRESS: 93\n",
      "PROGRESS: 93\n",
      "STATUS: Clustering finished (total inertia=784458.688)\n",
      "PROGRESS: 100\n",
      "DONE\n",
      "✅ Clustering completed. Total clusters: 8\n",
      "💾 Writing compressed CSV to: ../outputs/sample_1000000.csv.gz\n",
      "✅ Compressed CSV saved: ../outputs/sample_1000000.csv.gz\n",
      "🧩 Writing text summary... -> ../outputs/sample_1000000.txt\n",
      "✅ Cluster summaries saved: ../outputs/sample_1000000.txt\n",
      "⚠️ Large dataset detected, evaluating on sample of 200k entries...\n",
      "⚠️ Skipping Davies-Bouldin & Calinski-Harabasz (too large or sparse).\n",
      "📈 Cluster Evaluation: {'silhouette': np.float32(0.15688573), 'davies_bouldin': None, 'calinski_harabasz': None}\n",
      "🖼️ Skipping visualization due to dataset size (>200k).\n",
      "🎯 All done.\n"
     ]
    }
   ],
   "source": [
    "def cluster_logs(df, features, out_path, n_clusters):\n",
    "    \"\"\"\n",
    "    Cluster web server logs using feature embeddings and Bisecting KMeans,\n",
    "    optimized for large-scale datasets with gzip-compressed output.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame of log entries.\n",
    "        features (np.ndarray or scipy.sparse matrix): Feature matrix for clustering.\n",
    "        out_path (str): Output path ending with \".csv\" (example: \"outputs/result.csv\").\n",
    "        n_clusters (int): Number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Clustering process ===\n",
    "    print(\"🚀 Starting clustering process...\")\n",
    "    bkm = VerboseBisectingKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=42,\n",
    "        init=\"k-means++\",\n",
    "        n_init=5\n",
    "    )\n",
    "    bkm.fit_verbose(features)\n",
    "    labels = bkm.labels_\n",
    "    print(f\"✅ Clustering completed. Total clusters: {n_clusters}\")\n",
    "\n",
    "    # === 2. Attach cluster labels ===\n",
    "    df_label = df.copy()\n",
    "    df_label[\"cluster\"] = labels\n",
    "\n",
    "    # === 3. Define output paths ===\n",
    "    base, _ = os.path.splitext(out_path)\n",
    "    csv_path = f\"{base}.csv.gz\"\n",
    "    txt_path = f\"{base}.txt\"\n",
    "\n",
    "    # === 4. Save compressed CSV efficiently ===\n",
    "    chunk_size = 100_000\n",
    "    print(f\"💾 Writing compressed CSV to: {csv_path}\")\n",
    "    with open(csv_path, \"wb\") as f:  # gzip stream write\n",
    "        import gzip\n",
    "        with gzip.open(f, \"wt\", encoding=\"utf-8\", newline=\"\") as gz:\n",
    "            # Write header\n",
    "            df_label.head(0).to_csv(gz, index=False)\n",
    "            # Write in chunks\n",
    "            for i in range(0, len(df_label), chunk_size):\n",
    "                df_label.iloc[i:i+chunk_size].to_csv(gz, header=False, index=False)\n",
    "    print(f\"✅ Compressed CSV saved: {csv_path}\")\n",
    "\n",
    "    # === 5. Write summary TXT (truncated for huge data) ===\n",
    "    max_per_cluster = 1000 if len(df_label) > 1_000_000 else None\n",
    "    print(f\"🧩 Writing text summary... -> {txt_path}\")\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_data = df_label[df_label[\"cluster\"] == cluster_id]\n",
    "            f.write(f\"\\nCluster {cluster_id} ({len(cluster_data)} entries):\\n\")\n",
    "            if max_per_cluster:\n",
    "                cluster_data = cluster_data.head(max_per_cluster)\n",
    "                f.write(f\"  [Showing first {max_per_cluster} entries]\\n\")\n",
    "            for _, row in cluster_data.iterrows():\n",
    "                f.write(f\"  {row['method']} {row['url']} [{row['status']}]\\n\")\n",
    "    print(f\"✅ Cluster summaries saved: {txt_path}\")\n",
    "\n",
    "    # === 6. Evaluate clustering (sample-based for large data) ===\n",
    "    if len(df_label) > 200_000:\n",
    "        print(\"⚠️ Large dataset detected, evaluating on sample of 200k entries...\")\n",
    "        idx = np.random.choice(len(df_label), 200_000, replace=False)\n",
    "        sample_features = features[idx] if not issparse(features) else features[idx, :]\n",
    "        sample_labels = labels[idx]\n",
    "        metrics = evaluate_clusters(sample_features, sample_labels)\n",
    "    else:\n",
    "        metrics = evaluate_clusters(features, labels)\n",
    "\n",
    "    print(\"📈 Cluster Evaluation:\", metrics)\n",
    "\n",
    "    # === 7. Visualization (skip for huge data) ===\n",
    "    if len(df_label) <= 200_000:\n",
    "        base_png = f\"{base}_plot.png\"\n",
    "        visualize_clusters(features, labels, base_png, reduce_dim=True)\n",
    "        print(f\"🖼️ Cluster plot saved: {base_png}\")\n",
    "    else:\n",
    "        print(\"🖼️ Skipping visualization due to dataset size (>200k).\")\n",
    "\n",
    "    print(\"🎯 All done.\")\n",
    "\n",
    "    \n",
    "cluster_logs(df, final_features, output_dir, num_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
